import copy
import functools
import gym
import haiku as hk
import jax
import jax.numpy as jnp
import numpy as np
import optax
import os
import pickle
from collections import deque
from functools import partial
from jax import jit

from AntController.AntEnvironment import EpisodeData
from HaikuPredictor import HaikuPredictor


class EpisodeWithValue(EpisodeData):
    def __init__(self, episode_num, discount):
        super().__init__(episode_num)
        self.discount = discount
        self.values, self.advantages = [], None

    def get_discounted_rewards(self, discount):
        labels = []
        discounted_reward = np.asarray([0])
        for reward in self.rewards:
            discounted_reward = discounted_reward * self.discount + reward
            labels.append(discounted_reward)
        return np.asarray(labels[::-1])

    def get_critic_training_labels(self):
        return np.asarray(self.rewards) + np.pad(self.values, ((0, 1), (0, 0)), mode='constant')[1:] * self.discount

    def get_advantage(self):
        self.advantages = self.get_critic_training_labels()[:-1] - self.values[1:]
        self.advantages = self.advantages - np.mean(self.advantages)

    def get_per_action_advantage(self, action_space_size):
        self.get_advantage()
        per_action_advantage = []
        for i, action in enumerate(self.actions[:-1]):
            advantage = self.advantages[i][0]
            advantage_vector = np.ones(action_space_size) * (-advantage) / (action_space_size - 1)
            jax.ops.index_update(advantage_vector, action, advantage)
            per_action_advantage.append(advantage_vector)
        return np.asarray(per_action_advantage)

    def get_value_from_predictor(self, predictor):
        self.values = predictor.evaluate(np.asarray(self.states))
        return self.values

    def import_from_file(self, filepath):
        pass

    @classmethod
    def create_with_data(cls, episode_num, discount, states, rewards, actions):
        instance = cls(episode_num, discount)
        instance.states, instance.rewards, instance.actions = states, rewards, actions
        return instance


class HaikuActorCritic:
    def __init__(self, params, env):
        """Takes in a config which may be specified in YAML, and then creates the actor critic pair which can be used
        a environment. This class also handles the data which is being generated by the environments so we can have
        consistent batch sizes for the model training. """
        self.params = copy.deepcopy(params)
        self.actor = HaikuPredictor.generate_controller_from_config(params['actor_config'])
        self.value_critic = HaikuPredictor.generate_controller_from_config(params['critic_config'])
        self.episode_count = 0
        self.discount = params["discount"]
        self.env = env

        self.actor_training_queue = {
            "states": deque(maxlen=params["actor_queue_size"]),
            "per_action_advantage": deque(maxlen=params["actor_queue_size"])
        }
        self.critic_training_queue = {
            "states": deque(maxlen=params["critic_queue_size"]),
            "target_values": deque(maxlen=params["critic_queue_size"])
        }

    def train_critic(self):
        if len(self.critic_training_queue['states']) < self.params["critic_queue_size"]:
            return
        states = np.asarray(self.critic_training_queue['states'])
        target_values = np.asarray(self.critic_training_queue['target_values'])
        self.value_critic.train_batch(states, target_values)

    @functools.partial(jax.jit, static_argnums=0)
    def _train_actor(self, actor_params, states, per_action_advantage, optimizer_state):
        gradient = jax.grad(self._advantage_scaled_log_likelihoods)(actor_params, states, per_action_advantage)
        update, new_optimizer_state = self.actor.optimizer.update(
            gradient, optimizer_state)
        return optax.apply_updates(actor_params, update), new_optimizer_state

    def _advantage_scaled_log_likelihoods(self, actor_params, states, per_action_advantage):
        log_likelihoods = jnp.log(self.actor.net_t.apply(actor_params, states))
        return -jnp.sum(jnp.multiply(per_action_advantage,
                                     log_likelihoods))  # negative, since we're gradient ascending the likelihood

    def train_actor(self):
        if len(self.actor_training_queue['states']) < self.params["actor_queue_size"]:
            return
        states = np.asarray(self.actor_training_queue['states'])
        per_action_advantage = np.asarray(self.actor_training_queue['per_action_advantage'])
        self.actor.params, self.actor.optimizer_state = self._train_actor(self.actor.params,
                                                                          states,
                                                                          per_action_advantage,
                                                                          self.actor.optimizer_state)

    def get_sampled_action(self, state):
        action_distribution = np.array(self.actor.evaluate(state))
        action_distribution[0] = action_distribution[0] - sum(action_distribution) + 1  # This is a bug in numpy.
        return np.random.choice(self.env.action_space.n, p=action_distribution)

    def get_optimal_action(self, state):
        action_distribution = np.array(self.actor.evaluate(state))
        return np.argmax(action_distribution)

    def run_episode_with_actor(self, render, sampled):
        """Runs and episode with the actor"""
        episode_data = EpisodeWithValue(self.episode_count, self.discount)
        state = self.env.reset()
        while True:
            if render:
                self.env.render()
            if sampled:
                action = self.get_sampled_action(state)
            else:
                action = self.get_optimal_action(state)

            state_next, reward, terminal, _info = self.env.step(action)
            episode_data.values.append(aac.value_critic.evaluate(state))
            episode_data.add_step(state, action, [reward])
            state = state_next
            if terminal:
                break
        self.episode_count += 1
        return episode_data

    def add_episode_data_to_queue(self, episode_data):
        value_labels = episode_data.get_critic_training_labels()
        self.critic_training_queue['states'].extend(episode_data.states)
        self.critic_training_queue['target_values'].extend(value_labels)

        per_action_advantage = episode_data.get_per_action_advantage(self.env.action_space.n)
        self.actor_training_queue['states'].extend(episode_data.states[:-1])
        self.actor_training_queue['per_action_advantage'].extend(per_action_advantage)

    def run_episode_and_train(self, show, sampled):
        episode_data = self.run_episode_with_actor(show, sampled)
        self.add_episode_data_to_queue(episode_data)
        aac.train_critic()
        aac.train_actor()
        return episode_data


if __name__ == "__main__":
    ''' Imports a sample config, and then train actor critic on cartpole'''
    import yaml

    path = "AntController/configs/cartpole_aac_config.yaml"
    with open(path) as file:
        config = yaml.load(file, Loader=yaml.FullLoader)
    env = gym.make("CartPole-v1")
    aac = HaikuActorCritic(config, env)
    show = False

    while True:
        episode_data = aac.run_episode_and_train(show, not show)
        print("Episode {} with rewards {}".format(aac.episode_count, np.sum(episode_data.rewards)))
        if np.sum(episode_data.rewards) > 400 and aac.episode_count > 5000:
            show = True
